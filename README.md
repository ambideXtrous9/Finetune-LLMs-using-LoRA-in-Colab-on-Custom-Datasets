# Finetune-LLMs-using-LoRA-in-Colab-on-Custom-Datasets


https://replicate.delivery/pbxt/Ov6pvC22eKSYciScGRTJUKWQp9RBVR8gU4HKnioMtfwUPfEkA/out-0.png


## trainable params: 9,437,184 || all params: 2,859,194,368 || trainable%: 0.33006444422319176


[Causal LLMs and Seq2Seq Architectures](https://heidloff.net/article/causal-llm-seq2seq/#sequence-to-sequence)


[Understanding Causal LLM’s, Masked LLM’s, and Seq2Seq: A Guide to Language Model Training Approaches](https://medium.com/@tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)


[ "Compute Metrics" with Huggingface Question Answering](https://stackoverflow.com/questions/75744031/why-do-we-need-to-write-a-function-to-compute-metrics-with-huggingface-questio)
